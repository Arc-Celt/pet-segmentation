[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pet Segmentation with UNet",
    "section": "",
    "text": "In this report, I’ll present a UNet-based segmentation model trained on the Oxford-IIIT Pet Dataset. The model aims to accurately segment pets from images using a deep learning approach. The trained model has been saved to disk and will be loaded for evaluation and visualization.\n\n\n\n# Load necessary libraries\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom IPython.display import display, Markdown\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nUsing device: cuda\n\n\n\n\n\n\n\nThe Oxford-IIIT Pet Dataset consists of images of pets and their corresponding binary masks:\n\nImages: JPEG format (RGB)\nMasks: PNG format (binary mask with white for pet and black for background)\n\n\n\n\nTo efficiently load the data, I define a custom PetDataset class.\n\nclass PetDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.image_files = sorted(os.listdir(image_dir))\n        self.mask_files = sorted(os.listdir(mask_dir))\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.image_files[idx])\n        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n\n        # Load image and mask\n        image = Image.open(img_path).convert(\"RGB\")\n        mask = Image.open(mask_path).convert(\"L\")  # Grayscale mask\n\n        if self.transform:\n            image = self.transform(image)\n            mask = self.transform(mask)\n\n        return image, mask\n\n\n\n\nTo understand the data, let’s visualize a few samples from the training set, including images and their corresponding masks.\n\n\n\n\nData augmentation techniques are applied to enhance model generalization:\n\nResizing to a consistent size.\nNormalization for image tensors.\nNo normalization for masks to retain binary values.\n\n\nDATA_DIR = \"../data\"\nIMAGE_SIZE = 256\nBATCH_SIZE = 16\n\ndata_transforms = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n])\n\n# Mask transformations (no normalization)\nmask_transforms = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n])\n\n# Create datasets\nfull_dataset = PetDataset(\n    image_dir=os.path.join(DATA_DIR, \"images\"),\n    mask_dir=os.path.join(DATA_DIR, \"annotations\"),\n    transform=data_transforms\n)\n\n# Train-validation-test split\ntrain_size = int(0.7 * len(full_dataset))\nval_size = int(0.15 * len(full_dataset))\ntest_size = len(full_dataset) - train_size - val_size\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n    full_dataset, [train_size, val_size, test_size]\n)\n\n# Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n\n\n\n\nThe UNet model is a fully convolutional network designed to perform precise segmentation by combining:\n\nEncoder: Captures spatial context.\nBottleneck: Bridge between encoder and decoder.\nDecoder: Restores spatial resolution.\nSkip Connections: Preserve fine details by merging low-level features from the encoder.\n\n\n\n# DoubleConv Block\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n# UNet Model\nclass PetUNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1):\n        super(PetUNet, self).__init__()\n        self.enc1 = DoubleConv(in_channels, 64)\n        self.enc2 = DoubleConv(64, 128)\n        self.enc3 = DoubleConv(128, 256)\n        self.enc4 = DoubleConv(256, 512)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.bottleneck = DoubleConv(512, 1024)\n        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.dec4 = DoubleConv(1024, 512)\n        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.dec3 = DoubleConv(512, 256)\n        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.dec2 = DoubleConv(256, 128)\n        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.dec1 = DoubleConv(128, 64)\n        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(self.pool(enc1))\n        enc3 = self.enc3(self.pool(enc2))\n        enc4 = self.enc4(self.pool(enc3))\n        bottleneck = self.bottleneck(self.pool(enc4))\n        dec4 = self.dec4(torch.cat([self.up4(bottleneck), enc4], dim=1))\n        dec3 = self.dec3(torch.cat([self.up3(dec4), enc3], dim=1))\n        dec2 = self.dec2(torch.cat([self.up2(dec3), enc2], dim=1))\n        dec1 = self.dec1(torch.cat([self.up1(dec2), enc1], dim=1))\n        return torch.sigmoid(self.out_conv(dec1))\n\n\n\n\n\nNote: The model has already been trained and saved. The training code is provided for reproducibility but is not executed in this report.\n\n\nepochs = 20\nlearning_rate = 1e-4\n\nmodel = PetUNet(in_channels=3, out_channels=1).to(device)\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# results = trainer(model, criterion, optimizer, train_loader, valid_loader, epochs=epochs)\n# torch.save(model.state_dict(), \"results/models/pet_unet.pth\")\n\n\n\n\n\n# Load the saved model\nmodel = PetUNet().to(device)\nmodel.load_state_dict(torch.load(\"../results/models/pet_unet.pth\"))\nmodel.eval()\nprint(\"Model loaded successfully!\")\n\nModel loaded successfully!\n\n\n\n\n\nEvaluate the model on the test set and visualize predictions to assess performance.\n\ntest_results = test(model, criterion, test_loader)\ntest_loss = round(test_results['test_loss'], 4)\ntest_dice = round(test_results['test_dice'], 4)\n\nTest Loss: 0.1025 | Test Dice: 0.9536\n\n\nLet’s compare our predicted masks with ground truth to assess model performance.\n\n\n\n\nThe UNet-based pet segmentation model achieved a Test Loss of 0.1025 and a Test Dice Score of 0.9536, indicating high accuracy in distinguishing pets from the background. The model demonstrated consistent performance across the dataset, showcasing the effectiveness of the UNet architecture for binary segmentation tasks.\nFuture work could involve experimenting with advanced architectures like UNet++ or Attention UNet, and leveraging transfer learning to improve generalization. Additionally, extending this approach to multi-class segmentation tasks could further enhance performance.\n\n\n\nParkhi, O. M., Vedaldi, A., Zisserman, A., & Jawahar, C. V. (2012). Cats and Dogs. In IEEE Conference on Computer Vision and Pattern Recognition."
  },
  {
    "objectID": "index.html#environment-setup",
    "href": "index.html#environment-setup",
    "title": "Pet Segmentation with UNet",
    "section": "",
    "text": "# Load necessary libraries\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom IPython.display import display, Markdown\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nUsing device: cuda"
  },
  {
    "objectID": "index.html#dataset-and-data-loading",
    "href": "index.html#dataset-and-data-loading",
    "title": "Pet Segmentation with UNet",
    "section": "",
    "text": "The Oxford-IIIT Pet Dataset consists of images of pets and their corresponding binary masks:\n\nImages: JPEG format (RGB)\nMasks: PNG format (binary mask with white for pet and black for background)\n\n\n\n\nTo efficiently load the data, I define a custom PetDataset class.\n\nclass PetDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.image_files = sorted(os.listdir(image_dir))\n        self.mask_files = sorted(os.listdir(mask_dir))\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.image_files[idx])\n        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n\n        # Load image and mask\n        image = Image.open(img_path).convert(\"RGB\")\n        mask = Image.open(mask_path).convert(\"L\")  # Grayscale mask\n\n        if self.transform:\n            image = self.transform(image)\n            mask = self.transform(mask)\n\n        return image, mask\n\n\n\n\nTo understand the data, let’s visualize a few samples from the training set, including images and their corresponding masks.\n\n\n\n\nData augmentation techniques are applied to enhance model generalization:\n\nResizing to a consistent size.\nNormalization for image tensors.\nNo normalization for masks to retain binary values.\n\n\nDATA_DIR = \"../data\"\nIMAGE_SIZE = 256\nBATCH_SIZE = 16\n\ndata_transforms = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n])\n\n# Mask transformations (no normalization)\nmask_transforms = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n])\n\n# Create datasets\nfull_dataset = PetDataset(\n    image_dir=os.path.join(DATA_DIR, \"images\"),\n    mask_dir=os.path.join(DATA_DIR, \"annotations\"),\n    transform=data_transforms\n)\n\n# Train-validation-test split\ntrain_size = int(0.7 * len(full_dataset))\nval_size = int(0.15 * len(full_dataset))\ntest_size = len(full_dataset) - train_size - val_size\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n    full_dataset, [train_size, val_size, test_size]\n)\n\n# Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
  },
  {
    "objectID": "index.html#model-architecture-unet",
    "href": "index.html#model-architecture-unet",
    "title": "Pet Segmentation with UNet",
    "section": "",
    "text": "The UNet model is a fully convolutional network designed to perform precise segmentation by combining:\n\nEncoder: Captures spatial context.\nBottleneck: Bridge between encoder and decoder.\nDecoder: Restores spatial resolution.\nSkip Connections: Preserve fine details by merging low-level features from the encoder.\n\n\n\n# DoubleConv Block\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n# UNet Model\nclass PetUNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1):\n        super(PetUNet, self).__init__()\n        self.enc1 = DoubleConv(in_channels, 64)\n        self.enc2 = DoubleConv(64, 128)\n        self.enc3 = DoubleConv(128, 256)\n        self.enc4 = DoubleConv(256, 512)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.bottleneck = DoubleConv(512, 1024)\n        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.dec4 = DoubleConv(1024, 512)\n        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.dec3 = DoubleConv(512, 256)\n        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.dec2 = DoubleConv(256, 128)\n        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.dec1 = DoubleConv(128, 64)\n        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(self.pool(enc1))\n        enc3 = self.enc3(self.pool(enc2))\n        enc4 = self.enc4(self.pool(enc3))\n        bottleneck = self.bottleneck(self.pool(enc4))\n        dec4 = self.dec4(torch.cat([self.up4(bottleneck), enc4], dim=1))\n        dec3 = self.dec3(torch.cat([self.up3(dec4), enc3], dim=1))\n        dec2 = self.dec2(torch.cat([self.up2(dec3), enc2], dim=1))\n        dec1 = self.dec1(torch.cat([self.up1(dec2), enc1], dim=1))\n        return torch.sigmoid(self.out_conv(dec1))"
  },
  {
    "objectID": "index.html#model-training",
    "href": "index.html#model-training",
    "title": "Pet Segmentation with UNet",
    "section": "",
    "text": "Note: The model has already been trained and saved. The training code is provided for reproducibility but is not executed in this report.\n\n\nepochs = 20\nlearning_rate = 1e-4\n\nmodel = PetUNet(in_channels=3, out_channels=1).to(device)\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# results = trainer(model, criterion, optimizer, train_loader, valid_loader, epochs=epochs)\n# torch.save(model.state_dict(), \"results/models/pet_unet.pth\")"
  },
  {
    "objectID": "index.html#loading-the-trained-model",
    "href": "index.html#loading-the-trained-model",
    "title": "Pet Segmentation with UNet",
    "section": "",
    "text": "# Load the saved model\nmodel = PetUNet().to(device)\nmodel.load_state_dict(torch.load(\"../results/models/pet_unet.pth\"))\nmodel.eval()\nprint(\"Model loaded successfully!\")\n\nModel loaded successfully!"
  },
  {
    "objectID": "index.html#model-evaluation",
    "href": "index.html#model-evaluation",
    "title": "Pet Segmentation with UNet",
    "section": "",
    "text": "Evaluate the model on the test set and visualize predictions to assess performance.\n\ntest_results = test(model, criterion, test_loader)\ntest_loss = round(test_results['test_loss'], 4)\ntest_dice = round(test_results['test_dice'], 4)\n\nTest Loss: 0.1025 | Test Dice: 0.9536\n\n\nLet’s compare our predicted masks with ground truth to assess model performance."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Pet Segmentation with UNet",
    "section": "",
    "text": "The UNet-based pet segmentation model achieved a Test Loss of 0.1025 and a Test Dice Score of 0.9536, indicating high accuracy in distinguishing pets from the background. The model demonstrated consistent performance across the dataset, showcasing the effectiveness of the UNet architecture for binary segmentation tasks.\nFuture work could involve experimenting with advanced architectures like UNet++ or Attention UNet, and leveraging transfer learning to improve generalization. Additionally, extending this approach to multi-class segmentation tasks could further enhance performance."
  },
  {
    "objectID": "index.html#reference",
    "href": "index.html#reference",
    "title": "Pet Segmentation with UNet",
    "section": "",
    "text": "Parkhi, O. M., Vedaldi, A., Zisserman, A., & Jawahar, C. V. (2012). Cats and Dogs. In IEEE Conference on Computer Vision and Pattern Recognition."
  }
]